{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"채용 공고 추천.ipynb","private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyPfOKVQMUpY+gCC9OXnvhOA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"fYRs5XxsHrsR"},"source":["%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcMcchG_CRkk"},"source":["import numpy as np\r\n","import pandas as pd\r\n","import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Yi1rPEBHnfN"},"source":["train_data = pd.read_csv('train.csv')\r\n","print(train_data)\r\n","print(train_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XmdN-_PXuwu2"},"source":["user_tag = pd.read_csv('user_tags.csv')\r\n","job_tag = pd.read_csv('job_tags.csv')\r\n","raw_tags = pd.read_csv('tags.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRdZnsRA-PVr"},"source":["print(user_tag.sample(5))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IR-iqGGX2Bbg"},"source":["print(train_data.loc[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f5f3tnI0A8nw"},"source":["#user1 = train_data[\"userID\"] == \"fe292163d06253b716e9a0099b42031d\"\r\n","#print(train_data[user1])    # fe292163d06253b716e9a0099b42031d라는 유저는 27개의 지원 데이터가 있음"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ekCaltR5DOlt"},"source":["users = user_tag[\"userID\"].unique()\r\n","print(users.size)    # 196명의 사용자 대상 6000개의 데이터 확인"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EW-6LJKinidp"},"source":["tags = raw_tags[\"tagID\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wi2wYPuJ-Uvh"},"source":["print(len(raw_tags['keyword'].unique()))\r\n","print(len(raw_tags['tagID'].unique()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8BexE17nkyTV"},"source":["users_mat = np.zeros(shape=(users.size, tags.size))    # 각각의 유저들이 가진 태그들을 0,1로 이루어진 vector로 표현한 행렬\r\n","print(users_mat.shape)\r\n","for userID, tagID in user_tag.values:\r\n","    users_mat[np.where(users == userID)[0][0]][np.where(tags == tagID)[0][0]] = 1\r\n","print(np.sum(users_mat[103]))\r\n","print(np.sum(users_mat[150]))\r\n","print(np.sum(users_mat[174]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CS8xpaPm0ndg"},"source":["jobs = job_tag[\"jobID\"].unique()\r\n","print(jobs.size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x_8zSxU2vKbL"},"source":["jobs_mat = np.zeros(shape=(jobs.size, tags.size))\r\n","print(jobs_mat.shape)\r\n","for jobID, tagID in job_tag.values:\r\n","    jobs_mat[np.where(jobs == jobID)[0][0]][np.where(tags == tagID)[0][0]] = 1\r\n","print(np.sum(jobs_mat[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ebkfHGwk_CyU"},"source":["print(train_data.applied.sum())\r\n","print(len(train_data.applied))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lKM-PKnIGJTA"},"source":["#print(train_data[['userID',\"jobID\"]])\r\n","testdd = train_data['userID'] + \"  \" + train_data['jobID']\r\n","print(len(pd.unique(testdd)))\r\n","#print(len(pd.unique(train_data[['userID',\"jobID\"]].values.ravel('K'))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-09hc6lzvMJS"},"source":["cossim 방식"]},{"cell_type":"code","metadata":{"id":"UcomGsiFXGmR"},"source":["from sklearn.metrics.pairwise import cosine_similarity\r\n","from sklearn.metrics.pairwise import euclidean_distances\r\n","cossim = cosine_similarity\r\n","eucdist = euclidean_distances"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RuDc2uBlT6jH"},"source":["bestmatch = -1\r\n","user = np.where(users == \"fe292163d06253b716e9a0099b42031d\")[0][0]\r\n","print(user)\r\n","for i in range(len(users_mat)):\r\n","    if user != i:\r\n","        sim = cossim([users_mat[user], users_mat[i]])[0][1]\r\n","        #print(sim)\r\n","        if bestmatch < sim:\r\n","            bestmatch = sim\r\n","            bestuser = i\r\n","print(bestuser)\r\n","print(bestmatch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PCpDGvIPYVXp"},"source":["import matplotlib.pyplot as plt\r\n","plt.bar(tags, users_mat[103], color='r', linewidth=2)\r\n","plt.show()\r\n","plt.bar(tags, users_mat[150], color='g', linewidth=2)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fYNBzKyDwhpH"},"source":["import seaborn as sns\r\n","simmat = np.ndarray(shape=(len(users), len(users)))\r\n","print(simmat.shape)\r\n","for i in range(len(users)):\r\n","    for j in range(len(users)):\r\n","        simmat[i][j] = cossim([users_mat[i], users_mat[j]])[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-uzHMH1z8Bt"},"source":["fig, ax = plt.subplots(figsize=(8,8))\r\n","sns.heatmap(simmat, ax=ax)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xls0VLnX0gm9"},"source":["#plt.imshow(simmat, cmap='hot', interpolation='nearest')\r\n","#plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2ro63DaTdYP"},"source":["print(raw_tags[users_mat[103] == 1].keyword)\r\n","print(raw_tags[users_mat[150] == 1].keyword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIW7aTDJTjxx"},"source":["filter = np.logical_and(np.array(users_mat[103] == 1), np.array(users_mat[150] == 1))\r\n","print(raw_tags[filter].keyword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"akax59bfPFR4"},"source":["print(users_mat[np.where(users == 'fe292163d06253b716e9a0099b42031d')[0][0]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nMITnOpBgOdG"},"source":["#print(users_mat[0])\r\n","#print(users_mat[1])\r\n","#print(jobs_mat[0])\r\n","#print(jobs_mat[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vlj8kh2GfsaE"},"source":["def bestuser(user):\r\n","    bestmatch = -1\r\n","#    user = np.where(users == userID)[0][0]\r\n","    for i in range(len(users_mat)):\r\n","        if user != i:\r\n","            sim = cossim([users_mat[user], users_mat[i]])[0][1]\r\n","            #print(sim)\r\n","            if bestmatch < sim:\r\n","                bestmatch = sim\r\n","                bestuser = i\r\n","    print(bestuser)\r\n","    print(bestmatch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqnIkSD_l5PP"},"source":["def bestjob(user):\r\n","    bestmatch = -1\r\n","#    user = np.where(users == userID)[0][0]\r\n","    for i in range(len(users_mat)):\r\n","        sim = cossim([users_mat[user], jobs_mat[i]])[0][1]\r\n","        #print(sim)\r\n","        if bestmatch < sim:\r\n","            bestmatch = sim\r\n","            bestuser = i\r\n","    print(bestuser)\r\n","    print(bestmatch)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"octMPMgqgz-Q"},"source":["def cotag_user(user1, user2):\r\n","    print('user1: ', raw_tags[users_mat[user1] == 1].keyword)\r\n","    print('user2: ', raw_tags[users_mat[user2] == 1].keyword)\r\n","    filter = np.logical_and(np.array(users_mat[user1] == 1), np.array(users_mat[user2] == 1))\r\n","    print(raw_tags[filter].keyword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCH1RM4jmK8m"},"source":["def cotag_job(user, job):\r\n","    print('user1: ', raw_tags[users_mat[user] == 1].keyword)\r\n","    print('user2: ', raw_tags[jobs_mat[job] == 1].keyword)\r\n","    filter = np.logical_and(np.array(users_mat[user] == 1), np.array(jobs_mat[job] == 1))\r\n","    print(raw_tags[filter].keyword)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7dXwMBvgcbG"},"source":["print(bestjob(22))\r\n","print(cotag_job(22, 37))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C11NnLDlgsMZ"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfRaPY6gwROS"},"source":["\"\"\"\r\n","class Net(torch.nn.module):\r\n","    def __init__(self):\r\n","        super.__init__(self)\r\n","        \r\n","    def forward(self, x):\r\n","\"\"\"     "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R27v0Dt_13nH"},"source":["gmf"]},{"cell_type":"code","metadata":{"id":"bEvK-bIe13VX"},"source":["#user가 job에 지원했는지 표현하는 행렬(했으면 2, 아니면 1, data가 없으면 0)\r\n","user_jobmat = np.full(shape=(len(users),len(jobs)), fill_value=0)\r\n","for idx in train_data[:5000].values:\r\n","    uid = idx[0]\r\n","    jid = idx[1]\r\n","    app = idx[2]+1\r\n","    user_jobmat[np.where(users == uid)[0][0]][np.where(jobs == jid)[0][0]] = app\r\n","#print(user_jobmat)\r\n","#print(user_jobmat.shape)\r\n","#print(user_jobmat[1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmLmUxm33vDc"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","class NCF(nn.Module):\r\n","    def __init__(self, n_user=196, n_job=733, n_feature=100, d_out=2, dp=0.02):\r\n","        super().__init__()\r\n","        self.relu = nn.ReLU()\r\n","        self.user_embed = nn.Linear(n_job, n_feature)\r\n","        self.job_embed = nn.Linear(n_user, n_feature)\r\n","        \r\n","        self.ncfLayer = nn.Sequential(\r\n","#            nn.Linear(n_feature*2, n_feature),\r\n","#            nn.ReLU(),\r\n","#            nn.Dropout(dp),\r\n","#            nn.BatchNorm1d(n_feature),\r\n","            nn.Linear(n_feature, 50),\r\n","            nn.ReLU(),\r\n","            nn.Dropout(dp),\r\n","            nn.BatchNorm1d(50),\r\n","            nn.Linear(50, d_out),\r\n","            nn.Linear(d_out, 1)    # BCELOSS적용시\r\n","        )\r\n","        \r\n","    def forward(self, user, job):\r\n","        user = self.user_embed(user)\r\n","        job = self.job_embed(job)\r\n","#        x = torch.cat((user, job), dim=1)\r\n","        x = torch.mul(user, job)\r\n","        x = self.ncfLayer(x)\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDLTf_GeCb5j"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8sDIshr97YZQ"},"source":["criterion = nn.BCEWithLogitsLoss()\r\n","model = NCF()\r\n","optim = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=1e-8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tVonBssCZzJ_"},"source":["데이터셋, 샘플러"]},{"cell_type":"code","metadata":{"id":"RPjmMffG8H2R"},"source":["class UJDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","#        self.user_jobmat = np.full(shape=(len(users),len(jobs)), fill_value=-1)\r\n","#        for idx in data:\r\n","#            uid = idx[0]\r\n","#            jid = idx[1]\r\n","#            app = idx[2]\r\n","#            self.user_jobmat[np.where(users == uid)[0][0]][np.where(jobs == jid)[0][0]] = app\r\n","        self.sample = np.ndarray(shape=(len(data), 3), dtype=int)\r\n","        for i in range(len(data)):\r\n","            self.sample[i][0] = np.where(users == data.iloc[i][0])[0][0]\r\n","            self.sample[i][1] = np.where(jobs == data.iloc[i][1])[0][0]\r\n","            self.sample[i][2] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.sample)\r\n","\r\n","    def __getitem__(self, i):\r\n","        return user_jobmat[self.sample[i][0],:], user_jobmat[:,self.sample[i][1]].T, self.sample[i][2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EtCfaDBIIZ3H"},"source":["train_train_data = UJDataset(train_data[:5000])\r\n","train_test_data = UJDataset(train_data[5000:6000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t0DLf_lVMXTO"},"source":["targets = train_data[:5000].applied\r\n","class_sample_count = np.array([len(np.where(targets==t)[0]) for t in np.unique(targets)])\r\n","weights = 1. / class_sample_count\r\n","samples_weights = weights[targets]\r\n","trainsampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n0RtAu2NXts8"},"source":["targets = train_data[5000:6000].applied\r\n","class_sample_count = np.array([len(np.where(targets==t)[0]) for t in np.unique(targets)])\r\n","weights = 1. / class_sample_count\r\n","samples_weights = weights[targets]\r\n","testsampler = torch.utils.data.sampler.WeightedRandomSampler(samples_weights, len(samples_weights), replacement=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLRgs7jlUD9i"},"source":["#샘플러 사용\r\n","trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=5,sampler=trainsampler)\r\n","testloader = torch.utils.data.DataLoader(train_test_data, batch_size=5, sampler=testsampler)\r\n","#print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JlnAw8zkbRXQ"},"source":["rfefws = next(iter(trainloader))\r\n","#print(rfefws[0].shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t14JrUHohjUn"},"source":["#랜덤\r\n","#trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=5,shuffle=True)\r\n","#testloader = torch.utils.data.DataLoader(train_test_data, batch_size=5, shuffle=True)\r\n","#print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6U3tRfLzRcDm"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7V64zWp17Aft"},"source":["import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZsfUnGxaAQE"},"source":["훈련 시작"]},{"cell_type":"code","metadata":{"id":"h7BO2FJNh376"},"source":["epoch = 10\r\n","train_loss = 0\r\n","all_losses = []\r\n","for eps in range(epoch):\r\n","    print('training', eps+1 , 'epoch')\r\n","    for i, data in enumerate(trainloader, 0):\r\n","        model.train()\r\n","        uid, jid, labels = data\r\n","#        uid = data[:,0]\r\n","#        jid = data[:,1]\r\n","#        labels = data[:,2]\r\n","#        uid = user_jobmat[uid,:]\r\n","#        jid = user_jobmat[:,jid]\r\n","#        labels = torch.tensor(labels, dtype=torch.long)\r\n","        optim.zero_grad()\r\n","#        input_user = users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]\r\n","#        input_job = jobs_mat[np.where(jobs == train_train_data.iloc[i][1])[0][0]]                    \r\n","#        input_data = torch.tensor([users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]])\r\n","#        label = torch.zeros(2)    # 0 -> [1,0], 1 -> [0, 1]\r\n","#        label[train_train_data.iloc[i][2]] = 1\r\n","#        label = train_train_data.iloc[i][2]\r\n","        answers = model(uid.float(), jid.float())\r\n","        labels =labels.unsqueeze(1).float()\r\n","        loss = criterion(answers, labels)\r\n","        loss.backward();\r\n","        optim.step()\r\n","        train_loss += loss.item()\r\n","#        print(answers)\r\n","#        print(labels)\r\n","\r\n","        if i % 100 == 99:    # print every 100 mini-batches\r\n","#            print(answers)\r\n","#            print(labels)\r\n","#            print(\"a: \", answers.T, \" l: \", labels.T)\r\n","            \r\n","#            _, predicted = torch.max(answers.data, 1)\r\n","#            print(\"p: \", predicted, \" l: \", labels.T)\r\n","#            time.sleep(1)\r\n","            print('[%d, %5d] loss: %.3f' %\r\n","                  (epoch + 1, i + 1, train_loss / 100))\r\n","            all_losses.append(train_loss)\r\n","            train_loss = 0.0\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNKgRFLjJYhH"},"source":["import matplotlib.pyplot as plt\r\n","plt.plot(np.arange(len(all_losses)), all_losses)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uvU0NedczeDC"},"source":["correct = 0\r\n","total = 0\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        uid, jid, labels = data\r\n","        labels = torch.tensor(labels, dtype=torch.long)\r\n","        outputs = model(uid.float(), jid.float())\r\n","#        print(\"o: \", outputs)\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","#        print(\"p: \", predicted)\r\n","#        print(\"l: \", labels)\r\n","#        print(\"a: \", outputs.T, \" p: \",  predicted, \" l: \", labels)\r\n","#        time.sleep(3)\r\n","#        total += labels.sum()\r\n","        total += labels.size(0)\r\n","#        correct += labels[predicted == labels].sum()\r\n","        correct += (predicted == labels).sum().item()\r\n","\r\n","print('Accuracy: %d %%' % (\r\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XF094jgZaG38"},"source":["tag 정보 반영"]},{"cell_type":"code","metadata":{"id":"YT4rSgPRaHRC"},"source":["class UJDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","#        self.user_jobmat = np.full(shape=(len(users),len(jobs)), fill_value=-1)\r\n","#        for idx in data:\r\n","#            uid = idx[0]\r\n","#            jid = idx[1]\r\n","#            app = idx[2]\r\n","#            self.user_jobmat[np.where(users == uid)[0][0]][np.where(jobs == jid)[0][0]] = app\r\n","        self.sample = np.ndarray(shape=(len(data), 3), dtype=int)\r\n","        for i in range(len(data)):\r\n","            self.sample[i][0] = np.where(users == data.iloc[i][0])[0][0]\r\n","            self.sample[i][1] = np.where(jobs == data.iloc[i][1])[0][0]\r\n","            self.sample[i][2] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.sample)\r\n","\r\n","    def __getitem__(self, i):\r\n","        return self.sample[i]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n8uSK0bMcZMa"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyMEu6c4adey"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","class NCFwithTag(nn.Module):\r\n","    def __init__(self, n_user=196, n_job=733, n_feature=100, d_out=2, dp=0.02):\r\n","        super().__init__()\r\n","        n_tags = 887; d_embed = 400; d_hidden = 200\r\n","        self.relu = nn.ReLU()\r\n","        self.user_embed = nn.Linear(n_job, n_feature)\r\n","        self.job_embed = nn.Linear(n_user, n_feature)\r\n","        \r\n","        self.ncfLayer = nn.Sequential(\r\n","#            nn.Linear(n_feature*2, n_feature),\r\n","#            nn.ReLU(),\r\n","#            nn.Dropout(dp),\r\n","#            nn.BatchNorm1d(n_feature),\r\n","            nn.Linear(n_feature, 50),\r\n","            nn.ReLU(),\r\n","            nn.Dropout(dp),\r\n","            nn.BatchNorm1d(50),\r\n","            nn.Linear(50, d_out),\r\n","            nn.Linear(d_out, 1)    # BCELOSS적용시\r\n","        )\r\n","        self.module1 = nn.Sequential(    # usertags embedding\r\n","        nn.Linear(n_tags, d_embed),\r\n","        nn.ReLU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_embed),\r\n","        nn.Linear(d_embed, d_hidden),\r\n","        nn.ReLU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_hidden),\r\n","        nn.Linear(d_hidden, n_feature),\r\n","        nn.ReLU(),\r\n","        nn.BatchNorm1d(n_feature)\r\n","        )\r\n","        \r\n","        # self.module2 = nn.Sequential(    # jobtags embedding\r\n","        # nn.Linear(n_tags, d_embed),\r\n","        # nn.ELU(),\r\n","        # nn.Dropout(dp),\r\n","        # nn.BatchNorm1d(d_embed),\r\n","        # nn.Linear(d_embed, d_hidden),\r\n","        # nn.ELU(),\r\n","        # nn.Dropout(dp),\r\n","        # nn.BatchNorm1d(d_hidden),\r\n","        # nn.Linear(d_hidden, n_feature),\r\n","        # nn.ELU(),\r\n","        # nn.BatchNorm1d(n_feature)\r\n","        # )\r\n","\r\n","    def forward(self, ulv, ut, jlv, jt):\r\n","        ulv = self.user_embed(ulv)\r\n","        jlv = self.job_embed(jlv)\r\n","        ut = self.module1(ut)\r\n","        jt = self.module1(jt)\r\n","#        x = torch.cat((user, job), dim=1)\r\n","        x = torch.mul(ulv, jlv)\r\n","        y = torch.mul(ut, jt)\r\n","        x = torch.mul(x, y)\r\n","        x = self.ncfLayer(x)\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ZMW3Fe4cjxe"},"source":["train_train_data = UJDataset(train_data[:5000])\r\n","train_test_data = UJDataset(train_data[5000:6000])\r\n","trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=5,sampler=trainsampler)\r\n","testloader = torch.utils.data.DataLoader(train_test_data, batch_size=5, sampler=testsampler)\r\n","print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iL_N1P8HalWV"},"source":["def datahandler(data):\r\n","    uid = data[:,0]\r\n","    jid = data[:,1]\r\n","#    label = data[:,2]\r\n","    ulv = user_jobmat[uid,:]    # user latent vector\r\n","    jlv = user_jobmat[:,jid].T\r\n","    ut = users_mat[uid]\r\n","    jt = jobs_mat[jid]\r\n","    return ulv, ut, jlv, jt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T8jtrZA8dONw"},"source":["criterion = nn.BCEWithLogitsLoss()\r\n","model = NCFwithTag()\r\n","optim = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=1e-8)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0hbiyVhgp9e"},"source":["epoch = 5\r\n","train_loss = 0\r\n","all_losses = []\r\n","for eps in range(epoch):\r\n","    print('training', eps+1 , 'epoch')\r\n","    for i, data in enumerate(trainloader, 0):\r\n","        model.train()\r\n","        labels = data[:,2]\r\n","#        uid = data[:,0]\r\n","#        jid = data[:,1]\r\n","#        labels = data[:,2]\r\n","#        uid = user_jobmat[uid,:]\r\n","#        jid = user_jobmat[:,jid]\r\n","#        labels = torch.tensor(labels, dtype=torch.long)\r\n","        optim.zero_grad()\r\n","#        input_user = users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]\r\n","#        input_job = jobs_mat[np.where(jobs == train_train_data.iloc[i][1])[0][0]]                    \r\n","#        input_data = torch.tensor([users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]])\r\n","#        label = torch.zeros(2)    # 0 -> [1,0], 1 -> [0, 1]\r\n","#        label[train_train_data.iloc[i][2]] = 1\r\n","#        label = train_train_data.iloc[i][2]\r\n","        ulv, ut, jlv, jt = datahandler(data)\r\n","        ulv = torch.as_tensor(ulv).float()\r\n","        ut = torch.as_tensor(ut).float()\r\n","        jlv = torch.as_tensor(jlv).float()\r\n","        jt = torch.as_tensor(jt).float()\r\n","        answers = model(ulv, ut, jlv, jt)\r\n","        labels =labels.unsqueeze(1).float()\r\n","        loss = criterion(answers, labels)\r\n","        loss.backward();\r\n","        optim.step()\r\n","        train_loss += loss.item()\r\n","#        print(answers)\r\n","#        print(labels)\r\n","\r\n","        if i % 100 == 99:    # print every 100 mini-batches\r\n","#            print(answers)\r\n","#            print(labels)\r\n","#            print(\"a: \", answers.T, \" l: \", labels.T)\r\n","            \r\n","#            _, predicted = torch.max(answers.data, 1)\r\n","#            print(\"p: \", predicted, \" l: \", labels.T)\r\n","#            time.sleep(1)\r\n","            print('[%d, %5d] loss: %.3f' %\r\n","                  (epoch + 1, i + 1, train_loss / 100))\r\n","            all_losses.append(train_loss)\r\n","            train_loss = 0.0\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dX2nGbaFl1Yn"},"source":["import matplotlib.pyplot as plt\r\n","plt.plot(np.arange(len(all_losses)), all_losses)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjObdQ3HljyF"},"source":["correct = 0\r\n","total = 0\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        ulv, ut, jlv, jt = datahandler(data)\r\n","        ulv = torch.as_tensor(ulv).float()\r\n","        ut = torch.as_tensor(ut).float()\r\n","        jlv = torch.as_tensor(jlv).float()\r\n","        jt = torch.as_tensor(jt).float()\r\n","        labels = data[:,2]\r\n","        outputs = model(ulv, ut, jlv, jt)\r\n","#        print(\"o: \", outputs)\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","#        print(\"p: \", predicted)\r\n","#        print(\"l: \", labels)\r\n","#        print(\"a: \", outputs.T, \" p: \",  predicted, \" l: \", labels)\r\n","#        time.sleep(3)\r\n","#        total += labels.sum()\r\n","        total += labels.size(0)\r\n","#        correct += labels[predicted == labels].sum()\r\n","        correct += (predicted == labels).sum().item()\r\n","\r\n","print('Accuracy: %d %%' % (\r\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWA502vQ26Dq"},"source":["간단한 신경망 생성"]},{"cell_type":"code","metadata":{"id":"ycgOekbjTvFH"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","class Net(nn.Module):\r\n","    def __init__(self, n_embed=887, d_embed=400, d_hidden=200, d_out=2, dp=0.02):  #887차원의 데이터를 300차원으로 임베딩, 256개 은닉층을 거쳐 10개 데이터 아웃\r\n","        super().__init__()\r\n","        self.relu = nn.ReLU()\r\n","        self.fc1 = nn.Linear(n_embed, d_embed)\r\n","        self.dropout1 = nn.Dropout(dp)\r\n","        self.bn1 = nn.BatchNorm1d(d_embed)\r\n","        self.fc2 = nn.Linear(d_embed, d_hidden)\r\n","        self.dropout2 = nn.Dropout(dp)\r\n","        self.bn2 = nn.BatchNorm1d(d_hidden)\r\n","        self.fc3 = nn.Linear(d_hidden, d_out)   \r\n","\r\n","    def forward(self, x):\r\n","        \r\n","        x = self.fc1(x)\r\n","        x = self.dropout1(x)\r\n","        x = self.relu(x)\r\n","        x = self.bn1(x)\r\n","        x = self.fc2(x)\r\n","        x = self.dropout2(x)\r\n","        x = self.relu(x)\r\n","        x = self.bn2(x)\r\n","        x = self.fc3(x)\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jOSnRM1eWvkq"},"source":["criterion = nn.CrossEntropyLoss()\r\n","model = Net()\r\n","optim = torch.optim.Adam(model.parameters(), lr=0.002)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f_IpEeHDOtda"},"source":["#train_train_data = train_data[:5000]\r\n","#train_test_data = train_data[5000:6000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RfT5Git34yZM"},"source":["class TagsMatDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","        self.samples = np.ndarray(shape=(len(data), len(tags)))\r\n","        self.labels = np.zeros(shape=(len(data)))\r\n","        for i in range(len(data)):\r\n","            self.samples[i] = users_mat[np.where(users == data.iloc[i][0])[0][0]]\r\n","            self.labels[i] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.samples)\r\n","\r\n","    def __getitem__(self, idx):\r\n","        return self.samples[idx], self.labels[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dd3FIOvb7lqj"},"source":["train_train_data = TagsMatDataset(train_data[:5000])\r\n","train_test_data = TagsMatDataset(train_data[5000:6000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kYtsljgb2s6r"},"source":["trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=5, shuffle=True)\r\n","\r\n","print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5x1-DA2Nj8Bt"},"source":["#tm_da, tm_la = next(iter(trainloader))\r\n","#print(tm_da.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XFryUxF3U_yy"},"source":["epoch = 5\r\n","train_loss = 0\r\n","for eps in range(epoch):\r\n","    print('training', eps , 'epoch')\r\n","    for i, data in enumerate(trainloader, 0):\r\n","        model.train();\r\n","        inputs, labels = data\r\n","        labels = torch.tensor(labels, dtype=torch.long)\r\n","        optim.zero_grad()\r\n","#        input_user = users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]\r\n","#        input_job = jobs_mat[np.where(jobs == train_train_data.iloc[i][1])[0][0]]                    \r\n","#        input_data = torch.tensor([users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]])\r\n","#        label = torch.zeros(2)    # 0 -> [1,0], 1 -> [0, 1]\r\n","#        label[train_train_data.iloc[i][2]] = 1\r\n","#        label = train_train_data.iloc[i][2]\r\n","        answers = model(inputs.float())\r\n","        loss = criterion(answers, labels)\r\n","        loss.backward();\r\n","        optim.step()\r\n","        train_loss += loss.item()\r\n","        if i % 500 == 499:    # print every 2000 mini-batches\r\n","            print('[%d, %5d] loss: %.3f' %\r\n","                  (epoch + 1, i + 1, train_loss / 2000))\r\n","            train_loss = 0.0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zgo9Wk-VM_lL"},"source":["testloader = torch.utils.data.DataLoader(train_test_data, batch_size=5, shuffle=True)\r\n","\r\n","correct = 0\r\n","total = 0\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        inputs, labels = data\r\n","        outputs = model(inputs.float())\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","        total += labels.size(0)\r\n","        correct += (predicted == labels).sum().item()\r\n","\r\n","print('Accuracy: %d %%' % (\r\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"35WrfYNgVunW"},"source":["print(train_data.groupby('userID').sum().applied)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nEo7ZXiVj_Qe"},"source":["user, job을 각각 임베딩한 다음 둘 사이의 유사도 계산"]},{"cell_type":"code","metadata":{"id":"P0pMWV3JjeZT"},"source":["class TagsMatDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","        self.samples = np.ndarray(shape=(len(data), 2, len(tags)))    # user, job에 대한 태그보유여부가 0, 1로 표시된 행렬\r\n","        self.labels = np.zeros(shape=(len(data)))\r\n","        for i in range(len(data)):\r\n","            self.samples[i][0] = users_mat[np.where(users == data.iloc[i][0])[0][0]]\r\n","            self.samples[i][1] = jobs_mat[np.where(jobs == data.iloc[i][1])[0][0]]\r\n","            self.labels[i] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.samples)\r\n","\r\n","    def __getitem__(self, idx):\r\n","        return self.samples[idx][0], self.samples[idx][0], self.labels[idx]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gyuBWwlPj-fQ"},"source":["train_train_data = TagsMatDataset(train_data[:5000])\r\n","train_test_data = TagsMatDataset(train_data[5000:6000])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"urq0fJvQT_TJ"},"source":["trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=10, shuffle=True)\r\n","testloader = torch.utils.data.DataLoader(train_test_data, batch_size=10, shuffle=True)\r\n","print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YAMkdy1IRPpi"},"source":["#def cosineSim(x, y):\r\n","#    return cossim(x, y)[0][1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y7XdxDt3kSYF"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","class Net(nn.Module):\r\n","    def __init__(self, n_tags=887, d_embed=400, d_hidden=200, n_feature=100, dp=0.01):  #887차원의 데이터를 300차원으로 임베딩, 256개 은닉층을 거쳐 10개 데이터 아웃\r\n","        super().__init__()\r\n","        self.module1 = nn.Sequential(    # usertags embedding\r\n","        nn.Linear(n_tags, d_embed),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_embed),\r\n","        nn.Linear(d_embed, d_hidden),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_hidden),\r\n","        nn.Linear(d_hidden, n_feature),\r\n","        nn.ELU(),\r\n","        nn.BatchNorm1d(n_feature)\r\n","        )\r\n","        \r\n","        self.module2 = nn.Sequential(    # jobtags embedding\r\n","        nn.Linear(n_tags, d_embed),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_embed),\r\n","        nn.Linear(d_embed, d_hidden),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_hidden),\r\n","        nn.Linear(d_hidden, n_feature),\r\n","        nn.ELU(),\r\n","        nn.BatchNorm1d(n_feature)\r\n","        )\r\n","        self.ncfLayer = nn.Sequential(\r\n","#            nn.Linear(n_feature*2, n_feature),\r\n","#            nn.ELU(),\r\n","#            nn.Dropout(dp),\r\n","#            nn.BatchNorm1d(n_feature),\r\n","            nn.Linear(n_feature, 50),\r\n","            nn.ELU(),\r\n","            nn.Dropout(dp),\r\n","            nn.BatchNorm1d(50),\r\n","            nn.Linear(50, 2)\r\n","        )\r\n","#        self.fc1 = nn.Linear(n_feature*2, n_feature)\r\n","#        self.fc2 = nn.Linear(n_feature, 50)\r\n","#        self.fc3 = nn.Linear(50, 2)\r\n","#        self.cosineSim = lambda x, y : cossim(x, y)[0][1]\r\n","#        self.cossim = nn.CosineSimilarity(dim=1, eps=1e-6)\r\n","#        self.bias = nn.Parameter(torch.zeros(1))\r\n","    def forward(self, userTags, jobTags):\r\n","#        userTags = x[][0]\r\n","#        jobTags = x[1]\r\n","        userTags = self.module1(userTags)\r\n","        jobTags = self.module2(jobTags)\r\n","#        x = torch.cat((userTags, jobTags), dim=1)\r\n","#        x = self.ncfLayer(x)\r\n","        x = torch.mul(userTags, jobTags)\r\n","        x = self.ncfLayer(x)\r\n","#        sim = self.cossim(userTags, jobTags)\r\n","#        x = [1-sim, sim]\r\n","#        with torch.no_grad():\r\n","#            x = torch.empty()\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gYjtqQHkSGxr"},"source":["criterion = nn.CrossEntropyLoss()\r\n","model = Net()\r\n","optim = torch.optim.Adam(model.parameters(), lr=0.006, weight_decay=1e-2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jYMVI1dhSQtA"},"source":["epoch = 4\r\n","train_loss = 0\r\n","all_losses = []\r\n","for eps in range(epoch):\r\n","    print('training', eps+1 , 'epoch')\r\n","    for i, data in enumerate(trainloader, 0):\r\n","        model.train()\r\n","        user, job, labels = data\r\n","        labels = torch.tensor(labels, dtype=torch.long)\r\n","        optim.zero_grad()\r\n","#        input_user = users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]\r\n","#        input_job = jobs_mat[np.where(jobs == train_train_data.iloc[i][1])[0][0]]                    \r\n","#        input_data = torch.tensor([users_mat[np.where(users == train_train_data.iloc[i][0])[0][0]]])\r\n","#        label = torch.zeros(2)    # 0 -> [1,0], 1 -> [0, 1]\r\n","#        label[train_train_data.iloc[i][2]] = 1\r\n","#        label = train_train_data.iloc[i][2]\r\n","        answers = model(user.float(), job.float())\r\n","        loss = criterion(answers, labels)\r\n","        loss.backward();\r\n","        optim.step()\r\n","        train_loss += loss.item()\r\n","        if i % 100 == 99:    # print every 100 mini-batches\r\n","            print('[%d, %5d] loss: %.3f' %\r\n","                  (epoch + 1, i + 1, train_loss / 100))\r\n","            all_losses.append(train_loss)\r\n","            train_loss = 0.0\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oEjZuESJrRoj"},"source":["import matplotlib.pyplot as plt\r\n","plt.plot(np.arange(len(all_losses)), all_losses)\r\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tsuWSZGbVmDP"},"source":["testloader = torch.utils.data.DataLoader(train_test_data, batch_size=5, shuffle=True)\r\n","\r\n","correct = 0\r\n","total = 0\r\n","with torch.no_grad():\r\n","    for data in testloader:\r\n","        user, job, labels = data\r\n","        labels = torch.tensor(labels, dtype=torch.long)\r\n","        outputs = model(user.float(), job.float())\r\n","        _, predicted = torch.max(outputs.data, 1)\r\n","        total += labels.size(0)\r\n","        correct += (predicted == labels).sum().item()\r\n","\r\n","print('Accuracy: %d %%' % (\r\n","    100 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zaEYrfRmQvdD"},"source":["데이터에 유저 id를 추가로 반영"]},{"cell_type":"code","metadata":{"id":"VmjBCRyDQvOw"},"source":["class TagsMatDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","        self.uid = data.userID\r\n","        self.samples = np.ndarray(shape=(len(data), 2, len(tags)))    # user, job에 대한 태그보유여부가 0, 1로 표시된 행렬\r\n","        self.labels = np.zeros(shape=(len(data)))\r\n","        for i in range(len(data)):\r\n","            self.samples[i][0] = users_mat[np.where(users == data.iloc[i][0])[0][0]]\r\n","            self.samples[i][1] = jobs_mat[np.where(jobs == data.iloc[i][1])[0][0]]\r\n","            self.labels[i] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.samples)\r\n","\r\n","    def __getitem__(self, idx):\r\n","        return self.uid[idx], self.samples[idx][0], self.samples[idx][0], self.labels[idx]\r\n","\r\n","train_train_data = TagsMatDataset(train_data[:5000])\r\n","train_test_data = TagsMatDataset(train_data[5000:6000])\r\n","trainloader = torch.utils.data.DataLoader(train_train_data, batch_size=10, shuffle=True)\r\n","testloader = torch.utils.data.DataLoader(train_test_data, batch_size=10, shuffle=True)\r\n","print(next(iter(trainloader)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iDBwpZUpRruS"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","class Net(nn.Module):\r\n","    def __init__(self, n_tags=887, d_embed=400, d_hidden=200, n_feature=100, dp=0.01):  #887차원의 데이터를 300차원으로 임베딩, 256개 은닉층을 거쳐 10개 데이터 아웃\r\n","        super().__init__()\r\n","        self.module1 = nn.Sequential(    # usertags embedding\r\n","        nn.Linear(n_tags, d_embed),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_embed),\r\n","        nn.Linear(d_embed, d_hidden),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_hidden),\r\n","        nn.Linear(d_hidden, n_feature),\r\n","        nn.ELU(),\r\n","        nn.BatchNorm1d(n_feature)\r\n","        )\r\n","        \r\n","        self.module2 = nn.Sequential(    # jobtags embedding\r\n","        nn.Linear(n_tags, d_embed),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_embed),\r\n","        nn.Linear(d_embed, d_hidden),\r\n","        nn.ELU(),\r\n","        nn.Dropout(dp),\r\n","        nn.BatchNorm1d(d_hidden),\r\n","        nn.Linear(d_hidden, n_feature),\r\n","        nn.ELU(),\r\n","        nn.BatchNorm1d(n_feature)\r\n","        )\r\n","        self.fc1 = nn.Linear(n_feature*2, 2)\r\n","        self.cosineSim = lambda x, y : cossim(x, y)[0][1]\r\n","        self.cossim = nn.CosineSimilarity(dim=1, eps=1e-6)\r\n","#        self.bias = nn.Parameter(torch.zeros(1))\r\n","    def forward(self, userTags, jobTags):\r\n","#        userTags = x[][0]\r\n","#        jobTags = x[1]\r\n","        userTags = self.module1(userTags)\r\n","        jobTags = self.module1(jobTags)\r\n","        x = torch.cat((userTags, jobTags), dim=1)\r\n","        x = self.fc1(x)\r\n","#        sim = self.cossim(userTags, jobTags)\r\n","#        x = [1-sim, sim]\r\n","#        with torch.no_grad():\r\n","#            x = torch.empty()\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F9EtF0IWx61C"},"source":["유저에게 회사 추천"]},{"cell_type":"code","metadata":{"id":"5u7srCdpyAQZ"},"source":["target_user = users_mat[40]    # 40번 유저에게 추천되는 상위 3개 회사 표시\r\n","print(target_user.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eIk24r2rxvo9"},"source":["# 가장 유사한 job 검색\r\n","\r\n","with torch.no_grad():\r\n","    bestmatch = -1\r\n","    for i in range(len(jobs_mat)):\r\n","        target_job = jobs_mat[i]\r\n","        target_job = target_job.repeat(5)\r\n","        output = model(target_user, target_job)\r\n","        output = nn.Softmax(output)\r\n","        if bestmatch < output[1]:\r\n","            bestmatch = output[1]\r\n","            bestjob = i"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkIV4zDZbor9"},"source":["deep averaging network 모델(단어의 순서를 고려하지 않고 단어의 임베딩을 평균을 취해 만든다)\r\n"]},{"cell_type":"code","metadata":{"id":"TJ1gnQWQckWV"},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","\r\n","\r\n","class DAN(nn.Module):\r\n","\r\n","    def __init__(self,\r\n","                 n_embed=10000,\r\n","                 d_embed=300,\r\n","                 d_hidden=256,\r\n","                 d_out=2,\r\n","                 dp=0.2,\r\n","                 embed_weight=None):\r\n","        super(DAN, self).__init__()\r\n","\r\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n","\r\n","        self.embed = nn.Embedding(n_embed, d_embed)\r\n","\r\n","        if embed_weight is not None:\r\n","            # embed_weight = inputs.vocab.vectors\r\n","            self.embed.weight.data.copy_(embed_weight)\r\n","            self.embed.weight.requires_grad = False\r\n","\r\n","        self.dropout1 = nn.Dropout(dp)\r\n","        self.bn1 = nn.BatchNorm1d(d_embed)\r\n","        self.fc1 = nn.Linear(d_embed, d_hidden)\r\n","        self.dropout2 = nn.Dropout(dp)\r\n","        self.bn2 = nn.BatchNorm1d(d_hidden)\r\n","        self.fc2 = nn.Linear(d_hidden, d_out)\r\n","\r\n","    def forward(self, batch):\r\n","        text = batch.text\r\n","        label = batch.label\r\n","\r\n","        x = self.embed(text)\r\n","\r\n","        x = x.mean(dim=0)\r\n","\r\n","        x = self.dropout1(x)\r\n","        x = self.bn1(x)\r\n","        x = self.fc1(x)\r\n","        x = self.dropout2(x)\r\n","        x = self.bn2(x)\r\n","        x = self.fc2(x)\r\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"njmX3-sT_U0P"},"source":["\"\"\"\r\n","for user in users:\r\n","    #print(user)\r\n","    usertagfilter = user_tag[\"userID\"].isin([user])\r\n","    taglist = user_tag[usertagfilter].tagID.values    # 각 유저에 대한 태그 확인\r\n","    if user == \"fe292163d06253b716e9a0099b42031d\":    # 해당 유저는 151개 태그가 존재(중복 포함)\r\n","        print(taglist.size)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VTwpWm6WDUs"},"source":["\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfF37fMfv-qT"},"source":["#print(train_data[:1])\r\n","#print(user_tag)\r\n","#print(train_data.columns)\r\n","for user in range(0, len(train_data)):\r\n","    id = train_data.loc[user, \"userID\"]\r\n","    if id == \"fe292163d06253b716e9a0099b42031d\":\r\n","        print(id)\r\n","        usertagfilter = user_tag[\"userID\"].isin([id])\r\n","        usertag = user_tag[usertagfilter].tagID.values\r\n","        print(usertag)\r\n","#        print(user_tag[\"userID\"].isin([id]))\r\n","#    print(user['userID'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M0Arnkq7QMrQ"},"source":["xptmxm\r\n"]},{"cell_type":"code","metadata":{"id":"Ob5VIplRPzo4"},"source":["def dot(list_a, list_b):\r\n","    return sum(a*b for a, b in zip(list_a, list_b))\r\n","def jaccard(list_a, list_b):\r\n","    return dot(list_a, list_b) / (sum(a for a in list_a) + sum(b for b in list_b) - dot(list_a, list_b))\r\n","def norm(list_a):\r\n","    return sum(a*a for a in list_a) ** 0.5\r\n","def cossine(list_a, list_b):\r\n","    return dot(list_a, list_b) / (norm(list_a) * norm(list_b))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fl43qgwPMztb"},"source":["a = torch.rand(5, 5)\r\n","b = torch.rand(5, 5)\r\n","\r\n","costest = nn.CosineSimilarity(dim = 1)\r\n","print(a[1], b[1])\r\n","print(costest(a, b))\r\n","print(cossine(a[0], b[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOhFNG_yRVzo"},"source":["print(train_data.userID)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbE0qcKvBMlA"},"source":["ㅊㄴㅊㄴㅊ"]},{"cell_type":"code","metadata":{"id":"Y_W4IgUsBNZc"},"source":["import torch\r\n","from engine import Engine\r\n","from utils import use_cuda\r\n","\r\n","\r\n","class GMF(torch.nn.Module):\r\n","    def __init__(self, config):\r\n","        super(GMF, self).__init__()\r\n","        self.num_users = users.size\r\n","        self.num_items = jobs.size\r\n","        self.latent_dim = 8\r\n","\r\n","        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\r\n","        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\r\n","\r\n","        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\r\n","        self.logistic = torch.nn.Sigmoid()\r\n","\r\n","    def forward(self, user_indices, item_indices):\r\n","        user_embedding = self.embedding_user(user_indices)\r\n","        item_embedding = self.embedding_item(item_indices)\r\n","        element_product = torch.mul(user_embedding, item_embedding)\r\n","        logits = self.affine_output(element_product)\r\n","        rating = self.logistic(logits)\r\n","        return rating\r\n","\r\n","    def init_weight(self):\r\n","        pass\r\n","\r\n","\r\n","class GMFEngine(Engine):\r\n","    \"\"\"Engine for training & evaluating GMF model\"\"\"\r\n","    def __init__(self, config):\r\n","        self.model = GMF(config)\r\n","#        if config['use_cuda'] is True:\r\n","#            use_cuda(True, config['device_id'])\r\n","#            self.model.cuda()\r\n","#        super(GMFEngine, self).__init__(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TSteGrmaBpnZ"},"source":["class TrainDataset(torch.utils.data.Dataset):\r\n","    def __init__(self, data):\r\n","        super().__init__()\r\n","        self.sample = np.ndarray(shape=(len(data), 3))\r\n","        for i in range(len(data)):\r\n","            self.sample[i][0] = \r\n","            self.sample[i][1] = np.where(jobs == data.iloc[i][1])[0][0]\r\n","            self.sample[i][2] = data.iloc[i][2]\r\n","    def __len__(self):\r\n","        return len(self.sample)\r\n","\r\n","    def __getitem__(self, i):\r\n","        return self.sample[i]"],"execution_count":null,"outputs":[]}]}